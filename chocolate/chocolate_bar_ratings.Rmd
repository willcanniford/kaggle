---
title: "Chocolate Bar Ratings: EDA"
author: "Will Canniford"
output: 
  html_document:
    number_sections: false
    toc: true
    toc_depth: 6
    highlight: tango
    theme: yeti
    code_folding: show
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
library(tidyverse)
library(ggplot2)
library(readr)
library(stringr)
library(lubridate)
```

***

# Load and clean the data
Firstly, let's read in the packages that I am going to be using during the analysis.
```{r eval=FALSE}
library(tidyverse)
library(ggplot2)
library(readr)
library(stringr)
library(lubridate)
```
  
The data here is provided to us in a csv file and kept in the input directory, which is inside the current working directory. I just simply load it here using the `read_csv` function from the `readr` package. 

```{r message = FALSE}
full_data <- read_csv('./input/flavors_of_cacao.csv')
```

Now that we have read in the data we should explore the data set to try and get an understanding about how the data is stored for us.   

We can check the dimensions of the data. 
```{r}
dim(full_data)
```
We seem to have just under 1,800 observations, or chocolates, and 9 variables. We can check the variables that we have by checking the column names of the data. 

```{r}
names(full_data)
```

If we look at the column names we can see that they contain spaces as well as some line breaks, which definintely isn't good, so I will remove them using `gsub`. 
```{r}
# Remove and replace spaces and line breaks
names(full_data) <- tolower(gsub(pattern = '[[:space:]+]', '_', names(full_data)))

# View the new column names 
names(full_data)
```

That is much more manageable now. I have things about the `janitor` package that deals with things like that, perhaps in a better way so that will be something that I explore in the future.  

View the first few rows of the data. 
```{r}
head(full_data)
```

It looks like the first row of the data is duplicated from the header, so I will remove that as it definitely isn't an observation. 
```{r}
full_data <- full_data[-1,] # Remove first row
```

Another piece of data cleaning that we should do is remove the _%_ sign from the cocoa_percent column. This will mean that we can have the column as numeric, which makes much more sense than having it as a string or category. 
  
```{r}
# Remove the % sign from the cocoa percent column
full_data$cocoa_percent <- sapply(full_data$cocoa_percent, function(x) gsub("%", "", x))
```

We can now retype the columns to convert the cocoa_percent to a numeric. 
```{r}
# Retype the columns
full_data <- type_convert(full_data)

# Have a look at the new data types
map_df(full_data, class)
```

We can convert the year column to a date type using the `lubridate` package. 
```{r}
# Mutate a column that converts the year into a date type 
full_data <- full_data %>%
  mutate(review_date = ymd(paste(review_date, 1, 1, sep="-")))
```
  
We can use the newly created date column that we mutated to create a simple summary table grouping by the year.  
```{r}
avg_ratings <- full_data %>% 
  group_by(review_date) %>% # Group by new date column
  summarise(avg_rating = mean(rating), n_ratings = n()) # Summary stats

# Print our new table
avg_ratings
```
  
Here we have created a basic summary table of the average ratings and the number of reviewed observations for each year in the range of our data.   

***

# Insights
## By year
### Faceted distribution of ratings
```{r}
ggplot(full_data, aes(x = rating, fill = as.factor(review_date))) + 
  geom_density(alpha = .5) + 
  facet_wrap(~ as.factor(review_date)) + 
  guides(fill = FALSE) + labs(x = 'Rating', y = 'Density')
```

While the peaks throughout the years seems to remain fairly constant, you can see that the most recent year doesn't have the spread of the first years.  
We can analyse this further by creating a summary table and visualisation while grouping by the year. 

### Spread of ratings

```{r}
standard_deviations <- 
  full_data %>% group_by(review_date) %>%
  summarise(count = n(), avg = mean(rating), sd = sd(rating)) 

standard_deviations
```

We seem to see that the standard deviation of rating seems to be decreasing as time passes. This is more clearly seen when you visualise the trend.  

I have included the number of reviews per year here using the `size` aesthetic of `geom_point` to give an idea of the sample size for each year as time passes. 

```{r}
standard_deviations %>% # Use our previous summary table
  ggplot(aes(review_date, sd)) + 
  geom_point(aes(size = count)) + # Change point size depending on number of reviews
  geom_line(size = .5) + 
  theme_minimal() + 
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') + 
  scale_size(breaks = seq(0,250,50), name = 'Number of ratings') + 
  labs(x = 'Date', 
       y = 'Standard deviation of ratings', 
       title = 'Standard deviation of review ratings over time')
```

Upon visualisation of the data we can see that 2017 might simply have a smaller spread due to the smaller number of ratings that have been taken in that year. As the number of ratings taken in the year increases, we say see it rise to near that of the previous 6/7 years, although there does appear to be a definite downward trend, implying that reviewers are giving less extreme scores. 

### Extreme ratings

```{r}
low_scores <- full_data %>% 
  group_by(review_date) %>%
  summarise(less_than_2.5 = sum(rating < 2.5), count = n(), perc = round(less_than_2.5 / count, 2))

low_scores
```

Having a closer look, by year, at the reviews that were given a rating of less than 2.5, you can clearly see that the number of those poor scores is going down, despite the number of reviews going up! 18% of reviews in 2006 (13/72) where less than 2.5, while only 5 reviews fell below that threshold since 2015!
  
Once again, plotting this trend makes it much easier to digest. 
  
```{r}
low_scores %>%
  ggplot() + geom_line(aes(review_date, perc * 100)) + 
  theme_minimal() + 
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') + 
  labs(x = 'Date', 
       y = 'Percentage',
       title = 'Percentage of ratings below 2.5 by year')

```
  
### Frequent ratings over time
So we have established that the spread of the ratings is getting smaller, and reviewers are giving less low scores below 2.5.  

We can further visualise these features of the data by exploring other plots. 

```{r}
full_data %>%
  ggplot(aes(review_date, rating)) + 
  geom_count(aes(colour = ..n..)) + 
  geom_line(data = avg_ratings, aes(review_date, avg_rating), colour = 'red') + # Use average ratings table from earlier
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') + 
  scale_size_continuous(name = 'Count') + scale_color_continuous(name = 'Count') + 
  expand_limits(y = 0) + # Expand limits to include 0
  theme_minimal() + 
  labs(x = 'Review year', y = 'Rating')
```

We can see that the spread of the reviews is definitely narrowing, with the lower tail retracting and reviews now generally falling between 3 and 4 in 2015 onwards.  

***
  
## Companies and Makers
### Company location
We can investigate whether the location of the company has an impact on the quality of the product.  
```{r message = FALSE}
full_data %>%
  group_by(company_location) %>% 
  filter(n() > 10) %>% 
  mutate(avg = mean(rating)) %>%
  ggplot() + 
  geom_boxplot(aes(reorder(company_location, avg), rating, fill = avg)) + 
  scale_fill_continuous(low = '#ffffcc', high = '#fc4e2a', name = "Average rating") + 
  coord_flip() + 
  theme_minimal() + 
  labs(x = 'Company Location', y = 'Rating') +
  expand_limits(y = c(0,5))
```
  
### Master makers
As the column in the data is company/maker, I will extract the maker so that we can compare the makers against one another to see if there are any master makers hidden in the data.  

Once I have split the company and maker from the column then I will select those makers that have at least 3 reviewed products.  
```{r warning = FALSE, message=FALSE}
colnames(full_data)[1] <- 'company_maker'
makers <- full_data %>%
  rowwise %>%
  mutate(maker = str_match(company_maker, '\\((.*)\\)')[2]) %>% 
  filter(!is.na(maker)) %>% # If a maker was noted
  group_by(maker) %>% # Grouping
  filter(n() >= 3) %>% # Minimum product filter
  mutate(avg = mean(rating)) # Get average rating 

ggplot(makers) + 
  geom_boxplot(aes(x = reorder(maker, rating, FUN = mean), y = rating, fill = avg)) + 
  labs(x = 'Maker', y = 'Rating') + 
  coord_flip() + 
  theme_minimal() + 
  scale_fill_continuous(name = "Average rating") +
  expand_limits(y = c(0,5))
```
It appears as those *Cinagra* is our master chocolatier.  

### Company records over time
I split the company/maker column again, this time selecting the company, and filter by those with at least 10 reviews.    
```{r warning= FALSE, message = FALSE}
companies <- full_data %>%
  rowwise() %>%
  mutate(company = str_trim(str_split(company_maker, '\\(')[[1]][1])) %>%
  group_by(company) %>% 
  filter(n() > 10) %>% 
  mutate(avg = mean(rating))
```

By plotting the count of reviews for ratings using the `size` aesthetic again, we are also able to see the distribution of scores for the companies.

```{r warning= FALSE, message = FALSE}
companies  %>%
  ggplot(aes(x = reorder(as.factor(company), rating, FUN = mean), y = rating)) + 
  geom_count(alpha = .1) + 
  geom_point(aes(x = as.factor(company), y = avg, colour = avg)) + 
  theme_minimal() + 
  coord_flip() + 
  labs(x = 'Company', y = 'Rating') + 
  scale_color_continuous(name = 'Average rating', breaks = seq(3,4,.25)) + 
  scale_size_continuous(name = 'Number of ratings', breaks = seq(0,14,2))
```

This gives us an overview of companies overall, but doesn't show us trends for those companies over time. We can investigate that using `facet_wrap` and plotting against the `review_date`. 

```{r warning=F, fig.width=11, fig.height=12, message = F}
# Which of these companies are improving
full_data %>%
  rowwise() %>%
  mutate(company = str_trim(str_split(company_maker, '\\(')[[1]][1])) %>%
  group_by(company) %>% 
  filter(n() > 10) %>% 
  group_by(company, review_date) %>%
  summarise(count = n(), avg = mean(rating)) %>%
  ungroup() %>%
  ggplot() + 
    geom_point(aes(x = review_date, y = avg, size = count, colour = as.factor(company), alpha = 0.05)) +
    geom_line(aes(x = review_date, y = avg, colour = as.factor(company))) + 
    facet_wrap(~ company, ncol = 3) + 
    scale_x_date(date_labels = '%Y', date_breaks = '2 year') + 
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    guides(colour = FALSE) + 
  labs(x = 'Year of review', y = 'Average rating', title = "Company average ratings over time") + 
    scale_size_continuous(breaks = seq(4, 16, 2), name = 'Number of reviews') + 
    scale_alpha(guide = F)
```  
  
Some appear to have shown improvements, but such increases can probably just be put down to small number of reviews for a given year. To see whether the company is truly improving we would require a greater number of product reviews for the current year. 

***
  
## Cocoa percent 
Let's investigate to see whether there is a relationship between cocoa percentage and the chocolate's rating. 
```{r}
full_data %>%
  ggplot(aes(x = cocoa_percent, y = rating)) +
  geom_jitter(alpha = .75) + 
  coord_cartesian(ylim = c(0,5)) +
  labs(x = 'Cocoa percentage', y = 'Rating') + 
  theme_minimal() + 
  geom_smooth(method = 'lm', se = FALSE, col = 'red')
```
  
There doesn't appear to be any relationship. 
  
***
  
## Broad bean origin
```{r}
# Look at the origins
unique(full_data$broad_bean_origin)

# Remove the ratings that don't have broad bean origins
beans <- full_data %>% filter(!is.na(broad_bean_origin), !nchar(broad_bean_origin) < 2)
```

### Most frequent bean origin
```{r}
# Most frequent broad bean origin
beans %>% 
  group_by(broad_bean_origin) %>% 
  filter(n() > 10) %>% 
  mutate(count = n()) %>%
  ggplot(aes(x = reorder(broad_bean_origin, count))) + 
  geom_bar() + 
  coord_flip() + 
  theme_minimal() + 
  labs(x = 'Bean origin', y = 'Count')
```

### Origin and ratings
```{r}
# There doesn't appear to be any relationship between broad bean origin and rating however
beans %>% 
  group_by(broad_bean_origin) %>% 
  filter(n() > 10) %>% 
  mutate(count = n()) %>%
  ggplot() + 
  geom_boxplot(aes(x = broad_bean_origin, y = rating)) + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + 
  labs(x = 'Broad bean origin', y = 'Rating')
```

  
### Companies and bean origin heatmap
```{r fig.width=10 }
# Use the subset with the most active companies to generate heatmap of interactions between companies and the bean origin
companies %>% 
  filter(!is.na(broad_bean_origin), !nchar(broad_bean_origin) < 2) %>%
  group_by(company, broad_bean_origin) %>%
  summarise(count = n()) %>%
  filter(count > 0) %>%
  ggplot() + 
  geom_tile(aes(broad_bean_origin, company, fill = count)) + 
  coord_equal() +
  scale_fill_continuous(name = 'Count') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5)) +
  theme(axis.text.y = element_text(size = 5)) + 
  labs(x = 'Broad bean origin', y = 'Company')
```

Thanks for taking the time to have a look at my analysis and visualisations.   
I am still looking to improve this kernel and would greatly appreciate any feedback or advice that you could share with me. 